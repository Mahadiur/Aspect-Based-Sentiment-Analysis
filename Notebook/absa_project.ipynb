{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "### **Aspect Based Sentiment Analysis**",
   "id": "4e34b9c66bdd13bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.053037Z",
     "start_time": "2025-10-21T05:52:43.050275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "\n",
    "''' Load all import Library and Framework '''\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import re\n",
    "import pytorch_lightning\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch import optim"
   ],
   "id": "72db5a34de129d1f",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Load Dataset by use Pandas**",
   "id": "3fca59b6c41ec315"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.076560Z",
     "start_time": "2025-10-21T05:52:43.066109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset path\n",
    "Root_dir = '/Users/mahadiur/Desktop/Bongodev MLops Projects/Aspect Based Sentiment Analysis/Data'\n",
    "test_dir = os.path.join(Root_dir, 'test.csv')\n",
    "train_dir = os.path.join(Root_dir, 'train.csv')\n",
    "\n",
    "# Load dataset\n",
    "Test_Dataset = pd.read_csv(test_dir)\n",
    "Train_Dataset = pd.read_csv(train_dir)\n",
    "\n",
    "# Check Dataset\n",
    "print(Test_Dataset.head())\n",
    "print(Train_Dataset.head())"
   ],
   "id": "91fea613175a80fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review          aspect sentiment\n",
      "0                   The bread is top notch as well .           bread  positive\n",
      "1  I have to say they have one of the fastest del...  delivery times  positive\n",
      "2      Food is always fresh and hot - ready to eat !            Food  positive\n",
      "3     Did I mention that the coffee is OUTSTANDING ?          coffee  positive\n",
      "4  Certainly not the best sushi in New York , how...           place  positive\n",
      "                                              review   aspect sentiment\n",
      "0              But the staff was so horrible to us .    staff  negative\n",
      "1  To be completely fair , the only redeeming fac...     food  positive\n",
      "2  The food is uniformly exceptional , with a ver...     food  positive\n",
      "3  The food is uniformly exceptional , with a ver...  kitchen  positive\n",
      "4  The food is uniformly exceptional , with a ver...     menu   neutral\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.082133Z",
     "start_time": "2025-10-21T05:52:43.079994Z"
    }
   },
   "cell_type": "code",
   "source": "Train_Dataset.columns",
   "id": "aa83a87d6c37d21e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'aspect', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Data (ABSA Part 1)**",
   "id": "727a44d1d7df587c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.091083Z",
     "start_time": "2025-10-21T05:52:43.089134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Ready single example '''\n",
    "index = 0\n",
    "text = Train_Dataset.iloc[index]\n",
    "review = text['review']\n",
    "review = review.lower()\n",
    "review = ' '.join(review.split())\n",
    "print(review)\n"
   ],
   "id": "d4a22bd3e2e91702",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the staff was so horrible to us .\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.099411Z",
     "start_time": "2025-10-21T05:52:43.097154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Normalize Function '''\n",
    "def Normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "Normalize(review)"
   ],
   "id": "f2fbf00fbfa14dc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but the staff was so horrible to us'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.106887Z",
     "start_time": "2025-10-21T05:52:43.104803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Word-level Tokenization '''\n",
    "def Tokenization(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "text = Normalize(review)\n",
    "Tokenization(text)"
   ],
   "id": "c1115407d505fa42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.122989Z",
     "start_time": "2025-10-21T05:52:43.111952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Vocabulary '''\n",
    "def Vocabulary(texts):\n",
    "    token_id = {\n",
    "        '<padding>': 0,\n",
    "        '<unknown>': 1\n",
    "    }\n",
    "    idx = 2\n",
    "    for text in texts :\n",
    "        text = Normalize(text)\n",
    "        text = Tokenization(text)\n",
    "        for token in text:\n",
    "            if token_id.get(token) is None :\n",
    "                token_id[token] = idx\n",
    "                idx += 1\n",
    "    return token_id\n",
    "\n",
    "Token_2_id= Vocabulary(Train_Dataset['review'])\n",
    "print(len(Token_2_id))\n"
   ],
   "id": "14094e58a4560ead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.127943Z",
     "start_time": "2025-10-21T05:52:43.125737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert token to id '''\n",
    "def convert_token_2_id(tokens):\n",
    "    input_ids = [\n",
    "        Token_2_id.get(token, Token_2_id['<unknown>']) for token in tokens\n",
    "    ]\n",
    "    return input_ids\n",
    "\n",
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "review = text['review'] + 'hello'\n",
    "normalize = Normalize(review)\n",
    "Tokenize = Tokenization(normalize)\n",
    "input_id = convert_token_2_id(Tokenize)\n",
    "\n",
    "print(len(Token_2_id))\n",
    "print(Tokenize)\n",
    "print(input_id)"
   ],
   "id": "f8b2aa7d23289b29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n",
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'hello']\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 1]\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.134176Z",
     "start_time": "2025-10-21T05:52:43.132853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert All sentiment text to numeric '''\n",
    "Label = {\n",
    "    \"positive\": 0,\n",
    "    'neutral': 1,\n",
    "    'negative': 2\n",
    "}"
   ],
   "id": "4989b17775357a2c",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.141423Z",
     "start_time": "2025-10-21T05:52:43.139753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "pair_of_text = text['review'] + \" \" + text['sentiment']\n",
    "normalize = Normalize(pair_of_text)\n",
    "Tokenize = Tokenization(normalize)\n",
    "print(Tokenize)"
   ],
   "id": "2213ee1394c5d93e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'negative']\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.149246Z",
     "start_time": "2025-10-21T05:52:43.146857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Dataset class '''\n",
    "class ABSA_Dataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, texts):\n",
    "        text = Train_Dataset.iloc[texts]\n",
    "        review = text['review']\n",
    "        sentiment = text['sentiment']\n",
    "        aspect = text['aspect']\n",
    "        pair_of_text = review + ' ' + aspect\n",
    "        normalize = Normalize(pair_of_text)\n",
    "        tokenize = Tokenization(normalize)\n",
    "        input_ids = convert_token_2_id(tokenize)\n",
    "        label = Label[sentiment]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'label': label,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_func(batch):\n",
    "        # Batch wise load\n",
    "        batch_input_ids = [item['input_ids'] for item in batch]\n",
    "        batch_label = [item['label'] for item in batch]\n",
    "        # find max length input ids\n",
    "        max_len = max(len(input_ids) for input_ids in batch_input_ids)\n",
    "        # padding\n",
    "        pad_token_id = Token_2_id['<padding>']\n",
    "        # Same length every example\n",
    "        batch_padding_input_ids = [\n",
    "            input_ids + [pad_token_id] * (max_len - len(input_ids)) for input_ids in batch_input_ids\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            'batch_input_ids': torch.tensor(batch_padding_input_ids, dtype=torch.long),\n",
    "            'batch_label': torch.tensor(batch_label, dtype=torch.long)\n",
    "        }\n"
   ],
   "id": "267a0473930974b1",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.156245Z",
     "start_time": "2025-10-21T05:52:43.154255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Training = ABSA_Dataset(Train_Dataset)\n",
    "Training.__getitem__(0)"
   ],
   "id": "7478b148cecef8fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 3, 4, 5, 6, 7, 8, 9, 4], 'label': 2}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.163207Z",
     "start_time": "2025-10-21T05:52:43.161164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' ABSA DataModule '''\n",
    "\n",
    "class ABSA_Datamodule(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, test_path, train_path, batch_size):\n",
    "        super().__init__()\n",
    "        # Dataset path\n",
    "        self.test_path = test_path\n",
    "        self.train_path = train_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load Dataset by path\n",
    "        train_dataset = pd.read_csv(self.train_path)\n",
    "        test_dataset = pd.read_csv(self.test_path)\n",
    "\n",
    "        # build vocabulary\n",
    "        self.Vocabulary = convert_token_2_id(train_dataset['review'])\n",
    "\n",
    "        # return dataset\n",
    "        self.train_set = ABSA_Dataset(train_dataset)\n",
    "        self.test_set = ABSA_Dataset(test_dataset)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=ABSA_Dataset.collate_func\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=ABSA_Dataset.collate_func\n",
    "        )\n",
    "\n"
   ],
   "id": "b4bc1e58a38b73c0",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.173724Z",
     "start_time": "2025-10-21T05:52:43.167093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Module = ABSA_Datamodule(\n",
    "    test_path=test_dir,\n",
    "    train_path=train_dir,\n",
    "    batch_size=32\n",
    ")\n",
    "Module.setup()"
   ],
   "id": "8a5562e23a00bf",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Model (ABSA Part 2)**",
   "id": "934ef3ccb6135363"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.178825Z",
     "start_time": "2025-10-21T05:52:43.175938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ABSA_Model(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, vocabulary_size, num_labels = 3):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=vocabulary_size,\n",
    "            embedding_dim=256\n",
    "        )\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.LSTM_layer = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=512,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc_layer = nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=num_labels\n",
    "        )\n",
    "\n",
    "        # cost function\n",
    "        self.cost_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding_layer(x)\n",
    "        lstm_out, _ = self.LSTM_layer(embedding)\n",
    "        logits = self.fc_layer(lstm_out)\n",
    "        return logits\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['batch_input_ids']\n",
    "        labels = batch['batch_label']\n",
    "        logits = self(input_ids)\n",
    "        loss = self.cost_func(logits, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        acc = self.compute_accuracy(logits, labels)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['batch_input_ids']\n",
    "        labels = batch['batch_label']\n",
    "        logits = self(input_ids)\n",
    "        loss = self.cost_func(logits, labels)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        acc = self.compute_accuracy(logits, labels)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=3e-4)\n",
    "    def compute_metrics(self, logits, labels):\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        accuracy = Accuracy(task='multiclass', num_classes=3)"
   ],
   "id": "cc3a61292fe3abfd",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:52:43.191545Z",
     "start_time": "2025-10-21T05:52:43.180928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Model = ABSA_Model(\n",
    "    vocabulary_size=len(Token_2_id),\n",
    "    num_labels=3,\n",
    ")"
   ],
   "id": "5d0fef1540382447",
   "outputs": [],
   "execution_count": 147
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
