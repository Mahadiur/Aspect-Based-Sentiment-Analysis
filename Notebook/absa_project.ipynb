{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "### **Aspect Based Sentiment Analysis**",
   "id": "4e34b9c66bdd13bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.739262Z",
     "start_time": "2025-10-21T05:27:48.736134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Load all import Library and Framework '''\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import re\n",
    "import pytorch_lightning\n",
    "from torch.utils.data import DataLoader"
   ],
   "id": "72db5a34de129d1f",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Load Dataset by use Pandas**",
   "id": "3fca59b6c41ec315"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.767785Z",
     "start_time": "2025-10-21T05:27:48.756175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset path\n",
    "Root_dir = '/Users/mahadiur/Desktop/Bongodev MLops Projects/Aspect Based Sentiment Analysis/Data'\n",
    "test_dir = os.path.join(Root_dir, 'test.csv')\n",
    "train_dir = os.path.join(Root_dir, 'train.csv')\n",
    "\n",
    "# Load dataset\n",
    "Test_Dataset = pd.read_csv(test_dir)\n",
    "Train_Dataset = pd.read_csv(train_dir)\n",
    "\n",
    "# Check Dataset\n",
    "print(Test_Dataset.head())\n",
    "print(Train_Dataset.head())"
   ],
   "id": "91fea613175a80fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review          aspect sentiment\n",
      "0                   The bread is top notch as well .           bread  positive\n",
      "1  I have to say they have one of the fastest del...  delivery times  positive\n",
      "2      Food is always fresh and hot - ready to eat !            Food  positive\n",
      "3     Did I mention that the coffee is OUTSTANDING ?          coffee  positive\n",
      "4  Certainly not the best sushi in New York , how...           place  positive\n",
      "                                              review   aspect sentiment\n",
      "0              But the staff was so horrible to us .    staff  negative\n",
      "1  To be completely fair , the only redeeming fac...     food  positive\n",
      "2  The food is uniformly exceptional , with a ver...     food  positive\n",
      "3  The food is uniformly exceptional , with a ver...  kitchen  positive\n",
      "4  The food is uniformly exceptional , with a ver...     menu   neutral\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.779221Z",
     "start_time": "2025-10-21T05:27:48.776760Z"
    }
   },
   "cell_type": "code",
   "source": "Train_Dataset.columns",
   "id": "aa83a87d6c37d21e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'aspect', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Data (ABSA Part 1)**",
   "id": "727a44d1d7df587c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.790065Z",
     "start_time": "2025-10-21T05:27:48.788117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Ready single example '''\n",
    "index = 0\n",
    "text = Train_Dataset.iloc[index]\n",
    "review = text['review']\n",
    "review = review.lower()\n",
    "review = ' '.join(review.split())\n",
    "print(review)\n"
   ],
   "id": "d4a22bd3e2e91702",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the staff was so horrible to us .\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.807819Z",
     "start_time": "2025-10-21T05:27:48.805574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Normalize Function '''\n",
    "def Normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "Normalize(review)"
   ],
   "id": "f2fbf00fbfa14dc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but the staff was so horrible to us'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.816432Z",
     "start_time": "2025-10-21T05:27:48.814423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Word-level Tokenization '''\n",
    "def Tokenization(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "text = Normalize(review)\n",
    "Tokenization(text)"
   ],
   "id": "c1115407d505fa42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.832520Z",
     "start_time": "2025-10-21T05:27:48.821417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Vocabulary '''\n",
    "def Vocabulary(texts):\n",
    "    token_id = {\n",
    "        '<padding>': 0,\n",
    "        '<unknown>': 1\n",
    "    }\n",
    "    idx = 2\n",
    "    for text in texts :\n",
    "        text = Normalize(text)\n",
    "        text = Tokenization(text)\n",
    "        for token in text:\n",
    "            if token_id.get(token) is None :\n",
    "                token_id[token] = idx\n",
    "                idx += 1\n",
    "    return token_id\n",
    "\n",
    "Token_2_id= Vocabulary(Train_Dataset['review'])\n",
    "print(len(Token_2_id))\n"
   ],
   "id": "14094e58a4560ead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.837277Z",
     "start_time": "2025-10-21T05:27:48.835378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert token to id '''\n",
    "def convert_token_2_id(tokens):\n",
    "    input_ids = [\n",
    "        Token_2_id.get(token, Token_2_id['<unknown>']) for token in tokens\n",
    "    ]\n",
    "    return input_ids\n",
    "\n",
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "review = text['review'] + 'hello'\n",
    "normalize = Normalize(review)\n",
    "Tokenize = Tokenization(normalize)\n",
    "input_id = convert_token_2_id(Tokenize)\n",
    "\n",
    "print(len(Token_2_id))\n",
    "print(Tokenize)\n",
    "print(input_id)"
   ],
   "id": "f8b2aa7d23289b29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n",
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'hello']\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 1]\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.843093Z",
     "start_time": "2025-10-21T05:27:48.841581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert All sentiment text to numeric '''\n",
    "Label = {\n",
    "    \"positive\": 0,\n",
    "    'neutral': 1,\n",
    "    'negative': 2\n",
    "}"
   ],
   "id": "4989b17775357a2c",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.849877Z",
     "start_time": "2025-10-21T05:27:48.848178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "pair_of_text = text['review'] + \" \" + text['sentiment']\n",
    "normalize = Normalize(pair_of_text)\n",
    "Tokenize = Tokenization(normalize)\n",
    "print(Tokenize)"
   ],
   "id": "2213ee1394c5d93e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'negative']\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.857797Z",
     "start_time": "2025-10-21T05:27:48.855477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Dataset class '''\n",
    "class ABSA_Dataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, texts):\n",
    "        text = Train_Dataset.iloc[texts]\n",
    "        review = text['review']\n",
    "        sentiment = text['sentiment']\n",
    "        aspect = text['aspect']\n",
    "        pair_of_text = review + ' ' + aspect\n",
    "        normalize = Normalize(pair_of_text)\n",
    "        tokenize = Tokenization(normalize)\n",
    "        input_ids = convert_token_2_id(tokenize)\n",
    "        label = Label[sentiment]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'label': label,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_func(batch):\n",
    "        # Batch wise load\n",
    "        batch_input_ids = [item['input_ids'] for item in batch]\n",
    "        batch_label = [item['label'] for item in batch]\n",
    "        # find max length input ids\n",
    "        max_len = max(len(input_ids) for input_ids in batch_input_ids)\n",
    "        # padding\n",
    "        pad_token_id = Token_2_id['<padding>']\n",
    "        # Same length every example\n",
    "        batch_padding_input_ids = [\n",
    "            input_ids + [pad_token_id] * (max_len - len(input_ids)) for input_ids in batch_input_ids\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            'batch_input_ids': torch.tensor(batch_padding_input_ids, dtype=torch.long),\n",
    "            'batch_label': torch.tensor(batch_label, dtype=torch.long)\n",
    "        }\n"
   ],
   "id": "267a0473930974b1",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.867315Z",
     "start_time": "2025-10-21T05:27:48.865262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Training = ABSA_Dataset(Train_Dataset)\n",
    "Training.__getitem__(0)"
   ],
   "id": "7478b148cecef8fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 3, 4, 5, 6, 7, 8, 9, 4], 'label': 2}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.873862Z",
     "start_time": "2025-10-21T05:27:48.871866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' ABSA DataModule '''\n",
    "\n",
    "class ABSA_Datamodule(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, test_path, train_path, batch_size):\n",
    "        super().__init__()\n",
    "        # Dataset path\n",
    "        self.test_path = test_path\n",
    "        self.train_path = train_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load Dataset by path\n",
    "        train_dataset = pd.read_csv(self.train_path)\n",
    "        test_dataset = pd.read_csv(self.test_path)\n",
    "\n",
    "        # build vocabulary\n",
    "        self.Vocabulary = convert_token_2_id(train_dataset['review'])\n",
    "\n",
    "        # return dataset\n",
    "        self.train_set = ABSA_Dataset(train_dataset)\n",
    "        self.test_set = ABSA_Dataset(test_dataset)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=ABSA_Dataset.collate_func\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=ABSA_Dataset.collate_func\n",
    "        )\n",
    "\n"
   ],
   "id": "b4bc1e58a38b73c0",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:27:48.883735Z",
     "start_time": "2025-10-21T05:27:48.877799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Module = ABSA_Datamodule(\n",
    "    test_path=test_dir,\n",
    "    train_path=train_dir,\n",
    "    batch_size=32\n",
    ")\n",
    "Module.setup()"
   ],
   "id": "8a5562e23a00bf",
   "outputs": [],
   "execution_count": 131
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
