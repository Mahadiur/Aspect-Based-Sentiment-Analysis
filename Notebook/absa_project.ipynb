{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "### **Aspect Based Sentiment Analysis**",
   "id": "4e34b9c66bdd13bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:23:14.053775Z",
     "start_time": "2025-10-21T04:23:14.050600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from locale import normalize\n",
    "\n",
    "''' Load all import Library and Framework '''\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re"
   ],
   "id": "72db5a34de129d1f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Load Dataset by use Pandas**",
   "id": "3fca59b6c41ec315"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:23:14.074907Z",
     "start_time": "2025-10-21T04:23:14.062410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset path\n",
    "Root_dir = '/Users/mahadiur/Desktop/Bongodev MLops Projects/Aspect Based Sentiment Analysis/Data'\n",
    "test_dir = os.path.join(Root_dir, 'test.csv')\n",
    "train_dir = os.path.join(Root_dir, 'train.csv')\n",
    "\n",
    "# Load dataset\n",
    "Test_Dataset = pd.read_csv(test_dir)\n",
    "Train_Dataset = pd.read_csv(train_dir)\n",
    "\n",
    "# Check Dataset\n",
    "print(Test_Dataset.head())\n",
    "print(Train_Dataset.head())"
   ],
   "id": "91fea613175a80fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review          aspect sentiment\n",
      "0                   The bread is top notch as well .           bread  positive\n",
      "1  I have to say they have one of the fastest del...  delivery times  positive\n",
      "2      Food is always fresh and hot - ready to eat !            Food  positive\n",
      "3     Did I mention that the coffee is OUTSTANDING ?          coffee  positive\n",
      "4  Certainly not the best sushi in New York , how...           place  positive\n",
      "                                              review   aspect sentiment\n",
      "0              But the staff was so horrible to us .    staff  negative\n",
      "1  To be completely fair , the only redeeming fac...     food  positive\n",
      "2  The food is uniformly exceptional , with a ver...     food  positive\n",
      "3  The food is uniformly exceptional , with a ver...  kitchen  positive\n",
      "4  The food is uniformly exceptional , with a ver...     menu   neutral\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:23:14.082193Z",
     "start_time": "2025-10-21T04:23:14.079835Z"
    }
   },
   "cell_type": "code",
   "source": "Train_Dataset.columns",
   "id": "aa83a87d6c37d21e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'aspect', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Data (ABSA Part 1)**",
   "id": "727a44d1d7df587c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:23:14.090465Z",
     "start_time": "2025-10-21T04:23:14.088257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Ready single example '''\n",
    "index = 0\n",
    "text = Train_Dataset.iloc[index]\n",
    "review = text['review']\n",
    "review = review.lower()\n",
    "review = ' '.join(review.split())\n",
    "print(review)\n"
   ],
   "id": "d4a22bd3e2e91702",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the staff was so horrible to us .\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:23:14.098664Z",
     "start_time": "2025-10-21T04:23:14.096402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Normalize Function '''\n",
    "def Normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "Normalize(review)"
   ],
   "id": "f2fbf00fbfa14dc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but the staff was so horrible to us'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:23:14.105947Z",
     "start_time": "2025-10-21T04:23:14.103744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Word-level Tokenization '''\n",
    "def Tokenization(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "text = Normalize(review)\n",
    "Tokenization(text)"
   ],
   "id": "c1115407d505fa42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:23:14.120963Z",
     "start_time": "2025-10-21T04:23:14.110041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Vocabulary '''\n",
    "def Vocabulary(texts):\n",
    "    token_id = {\n",
    "        '<padding>': 0,\n",
    "        '<unknown>': 1\n",
    "    }\n",
    "    idx = 2\n",
    "    for text in texts :\n",
    "        text = Normalize(text)\n",
    "        text = Tokenization(text)\n",
    "        for token in text:\n",
    "            if token_id.get(token) is None :\n",
    "                token_id[token] = idx\n",
    "                idx += 1\n",
    "    return token_id\n",
    "\n",
    "Token_2_id= Vocabulary(Train_Dataset['review'])\n",
    "print(len(Token_2_id))\n"
   ],
   "id": "14094e58a4560ead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:24:08.447341Z",
     "start_time": "2025-10-21T04:24:08.442566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert token to id '''\n",
    "def convert_token_2_id(tokens):\n",
    "    input_ids = [\n",
    "        Token_2_id.get(token, Token_2_id['<unknown>']) for token in tokens\n",
    "    ]\n",
    "    return input_ids\n",
    "\n",
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "review = text['review'] + 'hello'\n",
    "normalize = Normalize(review)\n",
    "Tokenize = Tokenization(normalize)\n",
    "input_id = convert_token_2_id(Tokenize)\n",
    "\n",
    "print(len(Token_2_id))\n",
    "print(Tokenize)\n",
    "print(input_id)"
   ],
   "id": "f8b2aa7d23289b29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n",
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'hello']\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 1]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4989b17775357a2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
