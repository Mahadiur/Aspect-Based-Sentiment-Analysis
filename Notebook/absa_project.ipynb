{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "### **Aspect Based Sentiment Analysis**",
   "id": "4e34b9c66bdd13bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:09.947085Z",
     "start_time": "2025-10-21T05:05:09.943939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Load all import Library and Framework '''\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import re"
   ],
   "id": "72db5a34de129d1f",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Load Dataset by use Pandas**",
   "id": "3fca59b6c41ec315"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:09.963831Z",
     "start_time": "2025-10-21T05:05:09.952890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset path\n",
    "Root_dir = '/Users/mahadiur/Desktop/Bongodev MLops Projects/Aspect Based Sentiment Analysis/Data'\n",
    "test_dir = os.path.join(Root_dir, 'test.csv')\n",
    "train_dir = os.path.join(Root_dir, 'train.csv')\n",
    "\n",
    "# Load dataset\n",
    "Test_Dataset = pd.read_csv(test_dir)\n",
    "Train_Dataset = pd.read_csv(train_dir)\n",
    "\n",
    "# Check Dataset\n",
    "print(Test_Dataset.head())\n",
    "print(Train_Dataset.head())"
   ],
   "id": "91fea613175a80fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review          aspect sentiment\n",
      "0                   The bread is top notch as well .           bread  positive\n",
      "1  I have to say they have one of the fastest del...  delivery times  positive\n",
      "2      Food is always fresh and hot - ready to eat !            Food  positive\n",
      "3     Did I mention that the coffee is OUTSTANDING ?          coffee  positive\n",
      "4  Certainly not the best sushi in New York , how...           place  positive\n",
      "                                              review   aspect sentiment\n",
      "0              But the staff was so horrible to us .    staff  negative\n",
      "1  To be completely fair , the only redeeming fac...     food  positive\n",
      "2  The food is uniformly exceptional , with a ver...     food  positive\n",
      "3  The food is uniformly exceptional , with a ver...  kitchen  positive\n",
      "4  The food is uniformly exceptional , with a ver...     menu   neutral\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:09.972479Z",
     "start_time": "2025-10-21T05:05:09.970178Z"
    }
   },
   "cell_type": "code",
   "source": "Train_Dataset.columns",
   "id": "aa83a87d6c37d21e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'aspect', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Data (ABSA Part 1)**",
   "id": "727a44d1d7df587c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:09.979219Z",
     "start_time": "2025-10-21T05:05:09.976962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Ready single example '''\n",
    "index = 0\n",
    "text = Train_Dataset.iloc[index]\n",
    "review = text['review']\n",
    "review = review.lower()\n",
    "review = ' '.join(review.split())\n",
    "print(review)\n"
   ],
   "id": "d4a22bd3e2e91702",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the staff was so horrible to us .\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:09.985266Z",
     "start_time": "2025-10-21T05:05:09.982851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Normalize Function '''\n",
    "def Normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "Normalize(review)"
   ],
   "id": "f2fbf00fbfa14dc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but the staff was so horrible to us'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:09.992267Z",
     "start_time": "2025-10-21T05:05:09.989878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Word-level Tokenization '''\n",
    "def Tokenization(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "text = Normalize(review)\n",
    "Tokenization(text)"
   ],
   "id": "c1115407d505fa42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:10.006807Z",
     "start_time": "2025-10-21T05:05:09.995858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Vocabulary '''\n",
    "def Vocabulary(texts):\n",
    "    token_id = {\n",
    "        '<padding>': 0,\n",
    "        '<unknown>': 1\n",
    "    }\n",
    "    idx = 2\n",
    "    for text in texts :\n",
    "        text = Normalize(text)\n",
    "        text = Tokenization(text)\n",
    "        for token in text:\n",
    "            if token_id.get(token) is None :\n",
    "                token_id[token] = idx\n",
    "                idx += 1\n",
    "    return token_id\n",
    "\n",
    "Token_2_id= Vocabulary(Train_Dataset['review'])\n",
    "print(len(Token_2_id))\n"
   ],
   "id": "14094e58a4560ead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:10.014389Z",
     "start_time": "2025-10-21T05:05:10.012284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert token to id '''\n",
    "def convert_token_2_id(tokens):\n",
    "    input_ids = [\n",
    "        Token_2_id.get(token, Token_2_id['<unknown>']) for token in tokens\n",
    "    ]\n",
    "    return input_ids\n",
    "\n",
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "review = text['review'] + 'hello'\n",
    "normalize = Normalize(review)\n",
    "Tokenize = Tokenization(normalize)\n",
    "input_id = convert_token_2_id(Tokenize)\n",
    "\n",
    "print(len(Token_2_id))\n",
    "print(Tokenize)\n",
    "print(input_id)"
   ],
   "id": "f8b2aa7d23289b29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n",
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'hello']\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 1]\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:10.018911Z",
     "start_time": "2025-10-21T05:05:10.017362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert All sentiment text to numeric '''\n",
    "Label = {\n",
    "    \"Positive\": 0,\n",
    "    'Neutral': 1,\n",
    "    'Negative': 2\n",
    "}"
   ],
   "id": "4989b17775357a2c",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:10.024218Z",
     "start_time": "2025-10-21T05:05:10.022475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "pair_of_text = text['review'] + \" \" + text['sentiment']\n",
    "normalize = Normalize(pair_of_text)\n",
    "Tokenize = Tokenization(normalize)\n",
    "print(Tokenize)"
   ],
   "id": "2213ee1394c5d93e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'negative']\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:05:10.030071Z",
     "start_time": "2025-10-21T05:05:10.027279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Dataset class '''\n",
    "class ABSA_Dataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, texts):\n",
    "        text = self.dataset.iloc[texts]\n",
    "        review = text['review']\n",
    "        sentiment = text['sentiment']\n",
    "        aspect = text['aspect']\n",
    "        text = review + ' ' + sentiment\n",
    "        text = Normalize(text)\n",
    "        text = Tokenization(text)\n",
    "        input_ids = convert_token_2_id(text)\n",
    "        label = Label[aspect]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'label': label,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_func(batch):\n",
    "        # Batch wise load\n",
    "        batch_input_ids = [item['input_ids'] for item in batch]\n",
    "        batch_label = [item['label'] for item in batch]\n",
    "        # find max length input ids\n",
    "        max_len = max(len(input_ids) for input_ids in batch_input_ids)\n",
    "        # padding\n",
    "        pad_token_id = Token_2_id['<padding>']\n",
    "        # Same length every example\n",
    "        batch_padding_input_ids = [\n",
    "            input_ids + [pad_token_id] * (max_len - len(input_ids)) for input_ids in batch_input_ids\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            'batch_input_ids': torch.tensor(batch_padding_input_ids, dtype=torch.long),\n",
    "            'batch_label': torch.tensor(batch_label, dtype=torch.long)\n",
    "        }\n"
   ],
   "id": "267a0473930974b1",
   "outputs": [],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
