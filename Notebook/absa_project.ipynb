{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "### **Aspect Based Sentiment Analysis**",
   "id": "4e34b9c66bdd13bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.494405Z",
     "start_time": "2025-10-21T06:14:48.490255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "\n",
    "''' Load all import Library and Framework '''\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import re\n",
    "import pytorch_lightning\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch import optim"
   ],
   "id": "72db5a34de129d1f",
   "outputs": [],
   "execution_count": 304
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Load Dataset by use Pandas**",
   "id": "3fca59b6c41ec315"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.511202Z",
     "start_time": "2025-10-21T06:14:48.500757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset path\n",
    "Root_dir = '/Users/mahadiur/Desktop/Bongodev MLops Projects/Aspect Based Sentiment Analysis/Data'\n",
    "test_dir = os.path.join(Root_dir, 'test.csv')\n",
    "train_dir = os.path.join(Root_dir, 'train.csv')\n",
    "\n",
    "# Load dataset\n",
    "Test_Dataset = pd.read_csv(test_dir)\n",
    "Train_Dataset = pd.read_csv(train_dir)\n",
    "\n",
    "# Check Dataset\n",
    "print(Test_Dataset.head())\n",
    "print(Train_Dataset.head())"
   ],
   "id": "91fea613175a80fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review          aspect sentiment\n",
      "0                   The bread is top notch as well .           bread  positive\n",
      "1  I have to say they have one of the fastest del...  delivery times  positive\n",
      "2      Food is always fresh and hot - ready to eat !            Food  positive\n",
      "3     Did I mention that the coffee is OUTSTANDING ?          coffee  positive\n",
      "4  Certainly not the best sushi in New York , how...           place  positive\n",
      "                                              review   aspect sentiment\n",
      "0              But the staff was so horrible to us .    staff  negative\n",
      "1  To be completely fair , the only redeeming fac...     food  positive\n",
      "2  The food is uniformly exceptional , with a ver...     food  positive\n",
      "3  The food is uniformly exceptional , with a ver...  kitchen  positive\n",
      "4  The food is uniformly exceptional , with a ver...     menu   neutral\n"
     ]
    }
   ],
   "execution_count": 305
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.521107Z",
     "start_time": "2025-10-21T06:14:48.518482Z"
    }
   },
   "cell_type": "code",
   "source": "Train_Dataset.columns",
   "id": "aa83a87d6c37d21e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'aspect', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 306
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Data (ABSA Part 1)**",
   "id": "727a44d1d7df587c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.528338Z",
     "start_time": "2025-10-21T06:14:48.526062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Ready single example '''\n",
    "index = 0\n",
    "text = Train_Dataset.iloc[index]\n",
    "review = text['review']\n",
    "review = review.lower()\n",
    "review = ' '.join(review.split())\n",
    "print(review)\n"
   ],
   "id": "d4a22bd3e2e91702",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the staff was so horrible to us .\n"
     ]
    }
   ],
   "execution_count": 307
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.538294Z",
     "start_time": "2025-10-21T06:14:48.535672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Normalize Function '''\n",
    "def Normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "Normalize(review)"
   ],
   "id": "f2fbf00fbfa14dc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but the staff was so horrible to us'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 308
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.544565Z",
     "start_time": "2025-10-21T06:14:48.542618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Word-level Tokenization '''\n",
    "def Tokenization(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "text = Normalize(review)\n",
    "Tokenization(text)"
   ],
   "id": "c1115407d505fa42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us']"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 309
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.560752Z",
     "start_time": "2025-10-21T06:14:48.549085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Vocabulary '''\n",
    "def Vocabulary(texts):\n",
    "    token_id = {\n",
    "        '<padding>': 0,\n",
    "        '<unknown>': 1\n",
    "    }\n",
    "    idx = 2\n",
    "    for text in texts :\n",
    "        text = Normalize(text)\n",
    "        text = Tokenization(text)\n",
    "        for token in text:\n",
    "            if token_id.get(token) is None :\n",
    "                token_id[token] = idx\n",
    "                idx += 1\n",
    "    return token_id\n",
    "\n",
    "Token_2_id= Vocabulary(Train_Dataset['review'])\n",
    "print(len(Token_2_id))\n"
   ],
   "id": "14094e58a4560ead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n"
     ]
    }
   ],
   "execution_count": 310
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.569575Z",
     "start_time": "2025-10-21T06:14:48.567427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert token to id '''\n",
    "def convert_token_2_id(tokens):\n",
    "    input_ids = [\n",
    "        Token_2_id.get(token, Token_2_id['<unknown>']) for token in tokens\n",
    "    ]\n",
    "    return input_ids\n",
    "\n",
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "review = text['review'] + 'hello'\n",
    "normalize = Normalize(review)\n",
    "Tokenize = Tokenization(normalize)\n",
    "input_id = convert_token_2_id(Tokenize)\n",
    "\n",
    "print(len(Token_2_id))\n",
    "print(Tokenize)\n",
    "print(input_id)"
   ],
   "id": "f8b2aa7d23289b29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736\n",
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'hello']\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 1]\n"
     ]
    }
   ],
   "execution_count": 311
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.576321Z",
     "start_time": "2025-10-21T06:14:48.574810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Convert All sentiment text to numeric '''\n",
    "Label = {\n",
    "    \"positive\": 0,\n",
    "    'neutral': 1,\n",
    "    'negative': 2\n",
    "}"
   ],
   "id": "4989b17775357a2c",
   "outputs": [],
   "execution_count": 312
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.583153Z",
     "start_time": "2025-10-21T06:14:48.581227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = 0\n",
    "text = Train_Dataset.iloc[idx]\n",
    "pair_of_text = text['review'] + \" \" + text['sentiment']\n",
    "normalize = Normalize(pair_of_text)\n",
    "Tokenize = Tokenization(normalize)\n",
    "print(Tokenize)"
   ],
   "id": "2213ee1394c5d93e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'negative']\n"
     ]
    }
   ],
   "execution_count": 313
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.590588Z",
     "start_time": "2025-10-21T06:14:48.587368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Dataset class '''\n",
    "class ABSA_Dataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, texts):\n",
    "        text = Train_Dataset.iloc[texts]\n",
    "        review = text['review']\n",
    "        sentiment = text['sentiment']\n",
    "        aspect = text['aspect']\n",
    "        pair_of_text = review + ' ' + aspect\n",
    "        normalize = Normalize(pair_of_text)\n",
    "        tokenize = Tokenization(normalize)\n",
    "        input_ids = convert_token_2_id(tokenize)\n",
    "        label = Label[sentiment]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'label': label,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_func(batch):\n",
    "        # Batch wise load\n",
    "        batch_input_ids = [item['input_ids'] for item in batch]\n",
    "        batch_label = [item['label'] for item in batch]\n",
    "        # find max length input ids\n",
    "        max_len = max(len(input_ids) for input_ids in batch_input_ids)\n",
    "        # padding\n",
    "        pad_token_id = Token_2_id['<padding>']\n",
    "        # Same length every example\n",
    "        batch_padding_input_ids = [\n",
    "            input_ids + [pad_token_id] * (max_len - len(input_ids)) for input_ids in batch_input_ids\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            'batch_input_ids': torch.tensor(batch_padding_input_ids, dtype=torch.long),\n",
    "            'batch_label': torch.tensor(batch_label, dtype=torch.long)\n",
    "        }\n"
   ],
   "id": "267a0473930974b1",
   "outputs": [],
   "execution_count": 314
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.596120Z",
     "start_time": "2025-10-21T06:14:48.594149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Training = ABSA_Dataset(Train_Dataset)\n",
    "Training.__getitem__(0)"
   ],
   "id": "7478b148cecef8fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 3, 4, 5, 6, 7, 8, 9, 4], 'label': 2}"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 315
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.603109Z",
     "start_time": "2025-10-21T06:14:48.600703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' ABSA DataModule '''\n",
    "\n",
    "class ABSA_Datamodule(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, test_path, train_path, batch_size):\n",
    "        super().__init__()\n",
    "        # Dataset path\n",
    "        self.test_path = test_path\n",
    "        self.train_path = train_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load Dataset by path\n",
    "        train_dataset = pd.read_csv(self.train_path)\n",
    "        test_dataset = pd.read_csv(self.test_path)\n",
    "\n",
    "        # build vocabulary\n",
    "        self.Vocabulary = convert_token_2_id(train_dataset['review'])\n",
    "\n",
    "        # return dataset\n",
    "        self.train_set = ABSA_Dataset(train_dataset)\n",
    "        self.test_set = ABSA_Dataset(test_dataset)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=ABSA_Dataset.collate_func\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=ABSA_Dataset.collate_func\n",
    "        )\n",
    "\n"
   ],
   "id": "b4bc1e58a38b73c0",
   "outputs": [],
   "execution_count": 316
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.614531Z",
     "start_time": "2025-10-21T06:14:48.607250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Module = ABSA_Datamodule(\n",
    "    test_path=test_dir,\n",
    "    train_path=train_dir,\n",
    "    batch_size=8\n",
    ")\n",
    "Module.setup()"
   ],
   "id": "8a5562e23a00bf",
   "outputs": [],
   "execution_count": 317
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Model (ABSA Part 2)**",
   "id": "934ef3ccb6135363"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.620831Z",
     "start_time": "2025-10-21T06:14:48.617732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "class ABSA_Model(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, vocabulary_size, num_labels = 3):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=vocabulary_size,\n",
    "            embedding_dim=256\n",
    "        )\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.LSTM_layer = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=512,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc_layer = nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=num_labels\n",
    "        )\n",
    "\n",
    "        # cost function\n",
    "        self.cost_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding_layer(x)\n",
    "        lstm_out, _ = self.LSTM_layer(embedding)\n",
    "        logits = self.fc_layer(lstm_out[:,-1,:])\n",
    "        return logits\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['batch_input_ids'].to(device)\n",
    "        labels = batch['batch_label'].to(device)\n",
    "        logits = self(input_ids)\n",
    "        loss = self.cost_func(logits, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        acc = self.compute_metrics(logits, labels)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['batch_input_ids'].to(device)\n",
    "        labels = batch['batch_label'].to(device)\n",
    "        logits = self(input_ids)\n",
    "        loss = self.cost_func(logits, labels)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        acc = self.compute_metrics(logits, labels)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=3e-4)\n",
    "    def compute_metrics(self, logits, labels):\n",
    "        preds = logits.argmax(dim=1)\n",
    "        accuracy = Accuracy(task='multiclass', num_classes=3).to(device)\n",
    "        return accuracy(preds, labels)"
   ],
   "id": "cc3a61292fe3abfd",
   "outputs": [],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:14:48.633566Z",
     "start_time": "2025-10-21T06:14:48.622667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Model = ABSA_Model(\n",
    "    vocabulary_size=len(Token_2_id),\n",
    "    num_labels=3,\n",
    ")"
   ],
   "id": "5d0fef1540382447",
   "outputs": [],
   "execution_count": 319
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Training (ABSA Part 3)**",
   "id": "5fd395f9f98f992e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:17:08.276673Z",
     "start_time": "2025-10-21T06:14:48.636650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Training = pytorch_lightning.Trainer(\n",
    "    max_epochs=10,\n",
    ")\n",
    "Training.fit(Model, Module)"
   ],
   "id": "6ec2e8cd750f2154",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | embedding_layer | Embedding        | 956 K  | train\n",
      "1 | LSTM_layer      | LSTM             | 1.6 M  | train\n",
      "2 | fc_layer        | Linear           | 1.5 K  | train\n",
      "3 | cost_func       | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.140    Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 451/451 [00:14<00:00, 30.13it/s, v_num=10, train_loss=0.0878, train_acc=1.000] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 451/451 [00:14<00:00, 30.08it/s, v_num=10, train_loss=0.0878, train_acc=1.000]\n"
     ]
    }
   ],
   "execution_count": 320
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T06:17:11.022356Z",
     "start_time": "2025-10-21T06:17:08.431399Z"
    }
   },
   "cell_type": "code",
   "source": "Training.test(Model, Module)",
   "id": "629b38ab97617796",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:02<00:00, 54.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ\u001B[36m \u001B[0m\u001B[36m        test_acc         \u001B[0m\u001B[36m \u001B[0m‚îÇ\u001B[35m \u001B[0m\u001B[35m   0.9669347405433655    \u001B[0m\u001B[35m \u001B[0m‚îÇ\n",
       "‚îÇ\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m‚îÇ\u001B[35m \u001B[0m\u001B[35m   0.09328753501176834   \u001B[0m\u001B[35m \u001B[0m‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\">        Test metric        </span>‚îÉ<span style=\"font-weight: bold\">       DataLoader 0        </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.9669347405433655     </span>‚îÇ\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.09328753501176834    </span>‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.09328753501176834, 'test_acc': 0.9669347405433655}]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 321
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
